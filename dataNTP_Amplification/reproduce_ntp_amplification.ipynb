{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5c981ba9",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "# Case Study NTP Amplification\n",
    "\n",
    "Notebook case-study NTP Amplification."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5612ea9",
   "metadata": {},
   "source": [
    "## Setup ambiente"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48a18a37",
   "metadata": {},
   "source": [
    "## Caricamento dati"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6558d0f8",
   "metadata": {},
   "source": [
    "## Calcolo entropia e rapporti"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "31ef970b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generazione script compatibile con Jupyter Docker (/workspace/datasets/NTP_amplification/)\n",
    "notebook_code = \"\"\"\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.stats import entropy\n",
    "\n",
    "# Caricamento del dataset\n",
    "dataset_path = \"/workspace/datasets/NTP_amplification/a_ntp_amplification_dataset.csv\"\n",
    "df = pd.read_csv(dataset_path)\n",
    "\n",
    "# Calcolo entropia su finestre di 10 pacchetti\n",
    "window_size = 10\n",
    "df['window_id'] = df.index // window_size\n",
    "\n",
    "# Entropia sulla distribuzione delle etichette\n",
    "window_entropy = df.groupby('window_id')['label']\\\\\n",
    "    .apply(lambda x: entropy(x.value_counts(normalize=True), base=2))\\\\\n",
    "    .reset_index(name='entropy')\n",
    "\n",
    "# Merge e calcolo âˆ†H\n",
    "df = df.merge(window_entropy, on='window_id', how='left')\n",
    "df['delta_H'] = df['entropy'].diff()\n",
    "\n",
    "# Salvataggio dataset aggiornato\n",
    "output_path = \"/workspace/datasets/NTP_amplification/b_with_entropy.csv\"\n",
    "df.to_csv(output_path, index=False)\n",
    "\n",
    "print(f\"âœ… File salvato in: {output_path}\")\n",
    "df.head()\n",
    "\"\"\"\n",
    "\n",
    "script_path = \"/workspace/datasets/NTP_amplification/step_O2_entropy_pipeline.py\"\n",
    "with open(script_path, \"w\") as f:\n",
    "    f.write(notebook_code)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "cfa18eb4-4822-4e46-b3fa-2b2e48deac3f",
   "metadata": {},
   "source": [
    "### O1 RDF serialization script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f068a3e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Script for O1 - RDF serialization, compatible with /workspace environment\n",
    "rdf_script = \"\"\"\n",
    "from rdflib import Graph, Namespace, URIRef, Literal\n",
    "from rdflib.namespace import RDF, XSD\n",
    "import pandas as pd\n",
    "\n",
    "# Load input with entropy\n",
    "df = pd.read_csv(\"/workspace/datasets/NTP_amplification/b_with_entropy.csv\")\n",
    "\n",
    "# Prepare RDF graph\n",
    "g = Graph()\n",
    "NTP = Namespace(\"http://example.org/ntp#\")\n",
    "g.bind(\"ntp\", NTP)\n",
    "\n",
    "# Serialize each packet\n",
    "for _, row in df.iterrows():\n",
    "    packet_uri = URIRef(f\"http://example.org/ntp/packet/{row['packet_id']}\")\n",
    "    g.add((packet_uri, RDF.type, NTP.Packet))\n",
    "    g.add((packet_uri, NTP.hasBandwidth, Literal(int(row['bandwidth_MB']), datatype=XSD.integer)))\n",
    "    g.add((packet_uri, NTP.hasLabel, Literal(str(row['label']), datatype=XSD.string)))\n",
    "    \n",
    "    if not pd.isna(row['entropy']):\n",
    "        g.add((packet_uri, NTP.hasEntropy, Literal(float(row['entropy']), datatype=XSD.float)))\n",
    "    if not pd.isna(row['delta_H']):\n",
    "        g.add((packet_uri, NTP.hasDeltaH, Literal(float(row['delta_H']), datatype=XSD.float)))\n",
    "\n",
    "# Save RDF triples\n",
    "rdf_path = \"/workspace/datasets/NTP_amplification/O1_rdf_serialisation.ttl\"\n",
    "g.serialize(destination=rdf_path, format='turtle')\n",
    "print(f\"âœ… RDF serialization completed: {rdf_path}\")\n",
    "\"\"\"\n",
    "\n",
    "rdf_script_path = \"/workspace/datasets/NTP_amplification/step_O1_rdf_serialisation.py\"\n",
    "with open(rdf_script_path, \"w\") as f:\n",
    "    f.write(rdf_script)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bca93d30",
   "metadata": {},
   "source": [
    "## Defence Stack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4d40a0f6-2e8a-4a31-8612-c63fdb38a38f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Python script for Step O2: anomaly marking based on âˆ†H threshold\n",
    "step_O2_anomaly_script = \"\"\"\n",
    "import pandas as pd\n",
    "\n",
    "# Load entropy-enriched dataset\n",
    "df = pd.read_csv(\"/workspace/datasets/NTP_amplification/b_with_entropy.csv\")\n",
    "\n",
    "# Define anomaly threshold (as per paper, âˆ†H â‰¥ 1.5 bits)\n",
    "THRESHOLD_DH = 1.5\n",
    "\n",
    "# Flag anomalies\n",
    "df['anomaly_flag'] = df['delta_H'] >= THRESHOLD_DH\n",
    "\n",
    "# Save to new CSV\n",
    "output_path = \"/workspace/datasets/NTP_amplification/c_with_anomaly_flag.csv\"\n",
    "df.to_csv(output_path, index=False)\n",
    "\n",
    "print(f\"âœ… Anomaly marking completed. Output saved to: {output_path}\")\n",
    "print(f\"ðŸ” Total anomalies detected: {df['anomaly_flag'].sum()}\")\n",
    "\"\"\"\n",
    "\n",
    "anomaly_script_path = \"/workspace/datasets/NTP_amplification/step_O2_anomaly_flag.py\"\n",
    "with open(anomaly_script_path, \"w\") as f:\n",
    "    f.write(step_O2_anomaly_script)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "974ac5c9-9d82-4bf6-a825-601c8f9b1059",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Python script for Step O3 â€“ One-hot vectorization of ARNN input features\n",
    "step_O3_vectorization_script = \"\"\"\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "# Load dataset with anomaly flags\n",
    "df = pd.read_csv(\"/workspace/datasets/NTP_amplification/c_with_anomaly_flag.csv\")\n",
    "\n",
    "# Simulate network-level categorical features\n",
    "np.random.seed(42)\n",
    "df['srcIP'] = np.random.choice(['192.168.1.1', '10.0.0.5', '172.16.0.2'], size=len(df))\n",
    "df['dstIP'] = np.random.choice(['8.8.8.8', '1.1.1.1'], size=len(df))\n",
    "df['udp_port'] = np.random.choice([123, 53, 161], size=len(df))  # NTP, DNS, SNMP\n",
    "df['NTP_cmd'] = np.random.choice(['monlist', 'version', 'readvar'], size=len(df))\n",
    "\n",
    "# Apply one-hot encoding\n",
    "features_to_encode = df[['srcIP', 'dstIP', 'udp_port', 'NTP_cmd']]\n",
    "encoder = OneHotEncoder(sparse=False)\n",
    "encoded_array = encoder.fit_transform(features_to_encode)\n",
    "encoded_df = pd.DataFrame(encoded_array, columns=encoder.get_feature_names_out(features_to_encode.columns))\n",
    "\n",
    "# Merge encoded features back\n",
    "df_encoded = pd.concat([df.reset_index(drop=True), encoded_df], axis=1)\n",
    "\n",
    "# Save vectorized dataset\n",
    "output_path = \"/workspace/datasets/NTP_amplification/d_vectorized.csv\"\n",
    "df_encoded.to_csv(output_path, index=False)\n",
    "\n",
    "print(f\"âœ… Vectorization complete. Output saved to: {output_path}\")\n",
    "\"\"\"\n",
    "\n",
    "vectorization_script_path = \"/workspace/datasets/NTP_amplification/step_O3_vectorization.py\"\n",
    "with open(vectorization_script_path, \"w\") as f:\n",
    "    f.write(step_O3_vectorization_script)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f3411f3c-0a80-49ee-a0c8-449432f38047",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Python script for Step O4 â€“ LSTM-based ARNN simulation\n",
    "step_O4_arnn_script = \"\"\"\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Load vectorized dataset\n",
    "df = pd.read_csv(\"/workspace/datasets/NTP_amplification/d_vectorized.csv\")\n",
    "\n",
    "# Select input features for ARNN (one-hot encoded)\n",
    "input_features = [col for col in df.columns if col.startswith(('srcIP_', 'dstIP_', 'udp_port_', 'NTP_cmd_'))]\n",
    "X = df[input_features].values.astype(np.float32)\n",
    "\n",
    "# Simulate ARNN output risk scores for t+1 based on window\n",
    "np.random.seed(42)\n",
    "window_size = 5\n",
    "df['R_i'] = np.nan\n",
    "\n",
    "for i in range(window_size, len(df)):\n",
    "    df.loc[i, 'R_i'] = np.clip(0.2 + 0.75 * np.mean(X[i - window_size:i]), 0.2, 0.95)\n",
    "\n",
    "# Apply mitigation threshold as in paper: Ri > 0.55\n",
    "df['under_mitigation'] = df['R_i'] > 0.55\n",
    "\n",
    "# Save to file\n",
    "output_path = \"/workspace/datasets/NTP_amplification/e_with_risk_scores.csv\"\n",
    "df.to_csv(output_path, index=False)\n",
    "\n",
    "print(f\"âœ… ARNN simulation complete. Output saved to: {output_path}\")\n",
    "print(f\"ðŸš¨ Mitigation triggered on {df['under_mitigation'].sum()} packets.\")\n",
    "\"\"\"\n",
    "\n",
    "arnn_script_path = \"/workspace/datasets/NTP_amplification/step_O4_arnn_simulation.py\"\n",
    "with open(arnn_script_path, \"w\") as f:\n",
    "    f.write(step_O4_arnn_script)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c5a85cfc-a18e-4c3b-9995-4910bd8a4d4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Python script for Step O5 â€“ RDF risk graph creation\n",
    "step_O5_rdf_script = \"\"\"\n",
    "from rdflib import Graph, Namespace, URIRef, Literal\n",
    "from rdflib.namespace import RDF, XSD\n",
    "import pandas as pd\n",
    "\n",
    "# Load ARNN output\n",
    "df = pd.read_csv(\"/workspace/datasets/NTP_amplification/e_with_risk_scores.csv\")\n",
    "\n",
    "# Initialize RDF graph\n",
    "g = Graph()\n",
    "NTP = Namespace(\"http://example.org/ntp#\")\n",
    "g.bind(\"ntp\", NTP)\n",
    "\n",
    "# Add triples for risk scores and mitigation status\n",
    "for _, row in df.iterrows():\n",
    "    packet_uri = URIRef(f\"http://example.org/ntp/packet/{row['packet_id']}\")\n",
    "    g.add((packet_uri, RDF.type, NTP.Packet))\n",
    "    \n",
    "    if not pd.isna(row['R_i']):\n",
    "        g.add((packet_uri, NTP.hasRiskScore, Literal(round(row['R_i'], 4), datatype=XSD.float)))\n",
    "    \n",
    "    if row.get('under_mitigation', False):\n",
    "        g.add((packet_uri, NTP.underMitigation, Literal(True, datatype=XSD.boolean)))\n",
    "\n",
    "# Save graph to TTL\n",
    "rdf_path = \"/workspace/datasets/NTP_amplification/f_risk_graph.ttl\"\n",
    "g.serialize(destination=rdf_path, format=\"turtle\")\n",
    "\n",
    "print(f\"âœ… RDF risk graph generated and saved to: {rdf_path}\")\n",
    "\"\"\"\n",
    "\n",
    "rdf_graph_script_path = \"/workspace/datasets/NTP_amplification/step_O5_rdf_risk_graph.py\"\n",
    "with open(rdf_graph_script_path, \"w\") as f:\n",
    "    f.write(step_O5_rdf_script)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8f2f0d79-e511-4789-8c5f-16c9a4e39384",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Regenerate SPARQL query script after kernel reset\n",
    "step_O6_sparql_script = \"\"\"\n",
    "from rdflib import Graph\n",
    "\n",
    "# Load the RDF risk graph\n",
    "g = Graph()\n",
    "rdf_path = \"/workspace/datasets/NTP_amplification/f_risk_graph.ttl\"\n",
    "g.parse(rdf_path, format=\"ttl\")\n",
    "\n",
    "# Run SPARQL query to extract packets under mitigation\n",
    "query = '''\n",
    "PREFIX ntp: <http://example.org/ntp#>\n",
    "PREFIX xsd: <http://www.w3.org/2001/XMLSchema#>\n",
    "\n",
    "SELECT ?packet ?riskScore\n",
    "WHERE {\n",
    "  ?packet a ntp:Packet ;\n",
    "          ntp:hasRiskScore ?riskScore ;\n",
    "          ntp:underMitigation \"true\"^^xsd:boolean .\n",
    "}\n",
    "ORDER BY DESC(?riskScore)\n",
    "'''\n",
    "\n",
    "results = g.query(query)\n",
    "\n",
    "# Print results\n",
    "print(\"ðŸ” Packets under mitigation (Ri > 0.55):\")\n",
    "for row in results:\n",
    "    print(f\"{row.packet} -> Risk Score: {row.riskScore}\")\n",
    "\"\"\"\n",
    "\n",
    "sparql_script_path = \"/workspace/datasets/NTP_amplification/step_O6_sparql_query.py\"\n",
    "with open(sparql_script_path, \"w\") as f:\n",
    "    f.write(step_O6_sparql_script)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02701ac7-1062-4b71-a442-faf6176bae7a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
